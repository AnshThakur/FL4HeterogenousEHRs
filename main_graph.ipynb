{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engs2258/anaconda3/envs/graph/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from Model import GATT\n",
    "from loaders_FL import *\n",
    "from torch_geometric.nn.conv import GCN2Conv\n",
    "from utilsGHomo import *\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from MakeGraph import MakegraphH\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "random.seed(200)\n",
    "torch.manual_seed(200)\n",
    "np.random.seed(200)\n",
    "\n",
    "\n",
    "\n",
    "parameters = Params(num_sites = 10, \n",
    "                    num_rounds = 100,\n",
    "                    inner_epochs = 1,\n",
    "                    batch_size = 64,\n",
    "                    outer_lr = 0.001,\n",
    "                    inner_lr = 0.0001,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1865, 21)\n",
      "------------\n",
      "(161955, 28)\n",
      "------------\n",
      "(38717, 21)\n",
      "------------\n",
      "(95236, 28)\n",
      "------------\n",
      "Device: cuda:0\n",
      "GATT(\n",
      "  (conv1): GATConv(100, 128, heads=1)\n",
      "  (activation1): ReLU()\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      "  (fc): GATConv(128, 1, heads=1)\n",
      "  (activation2): Sigmoid()\n",
      ")\n",
      "------------------------------ STARTING TRAINING ROUND: 1 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7060...\n",
      "Node : 1.0 || Val AUC 0.4996\n",
      "Node : 2.0 || Val AUC 0.7338\n",
      "Node : 3.0 || Val AUC 0.5034\n",
      "------------------------------ STARTING TRAINING ROUND: 2 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7278...\n",
      "Node : 1.0 || Val AUC 0.5000\n",
      "Node : 2.0 || Val AUC 0.7338\n",
      "Node : 3.0 || Val AUC 0.5034\n",
      "------------------------------ STARTING TRAINING ROUND: 3 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7278...\n",
      "Node : 1.0 || Val AUC 0.5000\n",
      "Node : 2.0 || Val AUC 0.7459\n",
      "Node : 3.0 || Val AUC 0.5034\n",
      "------------------------------ STARTING TRAINING ROUND: 4 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7278...\n",
      "Node : 1.0 || Val AUC 0.5000\n",
      "Node : 2.0 || Val AUC 0.7459\n",
      "Node : 3.0 || Val AUC 0.5034\n",
      "------------------------------ STARTING TRAINING ROUND: 5 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7312...\n",
      "Node : 1.0 || Val AUC 0.5000\n",
      "Node : 2.0 || Val AUC 0.7459\n",
      "Node : 3.0 || Val AUC 0.5034\n",
      "------------------------------ STARTING TRAINING ROUND: 6 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7402...\n",
      "Node : 1.0 || Val AUC 0.5000\n",
      "Node : 2.0 || Val AUC 0.7555\n",
      "Node : 3.0 || Val AUC 0.5034\n",
      "------------------------------ STARTING TRAINING ROUND: 7 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7402...\n",
      "Node : 1.0 || Val AUC 0.5000\n",
      "Node : 2.0 || Val AUC 0.7555\n",
      "Node : 3.0 || Val AUC 0.5034\n",
      "------------------------------ STARTING TRAINING ROUND: 8 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7402...\n",
      "Node : 1.0 || Val AUC 0.7826\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.5034\n",
      "------------------------------ STARTING TRAINING ROUND: 9 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7402...\n",
      "Node : 1.0 || Val AUC 0.7826\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.5090\n",
      "------------------------------ STARTING TRAINING ROUND: 10 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7436...\n",
      "Node : 1.0 || Val AUC 0.7826\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.5163\n",
      "------------------------------ STARTING TRAINING ROUND: 11 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7436...\n",
      "Node : 1.0 || Val AUC 0.7826\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.5163\n",
      "------------------------------ STARTING TRAINING ROUND: 12 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7455...\n",
      "Node : 1.0 || Val AUC 0.7826\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.5163\n",
      "------------------------------ STARTING TRAINING ROUND: 13 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7489...\n",
      "Node : 1.0 || Val AUC 0.8110\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.5163\n",
      "------------------------------ STARTING TRAINING ROUND: 14 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7489...\n",
      "Node : 1.0 || Val AUC 0.8183\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.5163\n",
      "------------------------------ STARTING TRAINING ROUND: 15 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7489...\n",
      "Node : 1.0 || Val AUC 0.8183\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.5187\n",
      "------------------------------ STARTING TRAINING ROUND: 16 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7489...\n",
      "Node : 1.0 || Val AUC 0.8279\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.6844\n",
      "------------------------------ STARTING TRAINING ROUND: 17 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7489...\n",
      "Node : 1.0 || Val AUC 0.8279\n",
      "Node : 2.0 || Val AUC 0.7752\n",
      "Node : 3.0 || Val AUC 0.6844\n",
      "------------------------------ STARTING TRAINING ROUND: 18 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7489...\n",
      "Node : 1.0 || Val AUC 0.8279\n",
      "Node : 2.0 || Val AUC 0.7920\n",
      "Node : 3.0 || Val AUC 0.6844\n",
      "------------------------------ STARTING TRAINING ROUND: 19 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7591...\n",
      "Node : 1.0 || Val AUC 0.8279\n",
      "Node : 2.0 || Val AUC 0.7920\n",
      "Node : 3.0 || Val AUC 0.7523\n",
      "------------------------------ STARTING TRAINING ROUND: 20 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7591...\n",
      "Node : 1.0 || Val AUC 0.8279\n",
      "Node : 2.0 || Val AUC 0.7920\n",
      "Node : 3.0 || Val AUC 0.7523\n",
      "------------------------------ STARTING TRAINING ROUND: 21 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7591...\n",
      "Node : 1.0 || Val AUC 0.8336\n",
      "Node : 2.0 || Val AUC 0.7920\n",
      "Node : 3.0 || Val AUC 0.7523\n",
      "------------------------------ STARTING TRAINING ROUND: 22 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7591...\n",
      "Node : 1.0 || Val AUC 0.8336\n",
      "Node : 2.0 || Val AUC 0.7920\n",
      "Node : 3.0 || Val AUC 0.7523\n",
      "------------------------------ STARTING TRAINING ROUND: 23 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7593...\n",
      "Node : 1.0 || Val AUC 0.8336\n",
      "Node : 2.0 || Val AUC 0.7920\n",
      "Node : 3.0 || Val AUC 0.7523\n",
      "------------------------------ STARTING TRAINING ROUND: 24 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7593...\n",
      "Node : 1.0 || Val AUC 0.8336\n",
      "Node : 2.0 || Val AUC 0.7920\n",
      "Node : 3.0 || Val AUC 0.7523\n",
      "------------------------------ STARTING TRAINING ROUND: 25 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7593...\n",
      "Node : 1.0 || Val AUC 0.8336\n",
      "Node : 2.0 || Val AUC 0.7939\n",
      "Node : 3.0 || Val AUC 0.7523\n",
      "------------------------------ STARTING TRAINING ROUND: 26 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7600...\n",
      "Node : 1.0 || Val AUC 0.8336\n",
      "Node : 2.0 || Val AUC 0.8039\n",
      "Node : 3.0 || Val AUC 0.7523\n",
      "------------------------------ STARTING TRAINING ROUND: 27 ... ---------------------------------\n",
      "Node : 0.0 || Val AUC 0.7669...\n",
      "Node : 1.0 || Val AUC 0.8336\n",
      "Node : 2.0 || Val AUC 0.8039\n",
      "Node : 3.0 || Val AUC 0.7523\n",
      "------------------------------ STARTING TRAINING ROUND: 28 ... ---------------------------------\n",
      "Node : 0/4 training complete...\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 222\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39m# Loop to iteratively train each client locally\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, nodes):\n\u001b[0;32m--> 222\u001b[0m     TL[j], GRADS[j], NETS[j] \u001b[39m=\u001b[39m client_training(global_weights, Loaders, j, inner_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n\u001b[1;32m    223\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNode : \u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnodes\u001b[39m}\u001b[39;00m\u001b[39m training complete...\u001b[39m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m) \n\u001b[1;32m    225\u001b[0m \u001b[39m# Combine the grads from each client after a round of local training across all clients to get the meta grad\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 155\u001b[0m, in \u001b[0;36mclient_training\u001b[0;34m(global_weights, Loaders, client_id, inner_epochs, device)\u001b[0m\n\u001b[1;32m    151\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, label)\n\u001b[1;32m    153\u001b[0m \u001b[39m# print(loss)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m# backward pass to get gradients for optimization\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    158\u001b[0m inner_opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    161\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem() \n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [code]\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from Model import *\n",
    "from loaders_FL import *\n",
    "# from torch_geometric.nn.conv import GCN2Conv\n",
    "from utilsGHomo import *\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from MakeGraph import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "random.seed(210)\n",
    "torch.manual_seed(210)\n",
    "np.random.seed(210)\n",
    "\n",
    "\n",
    "\n",
    "parameters = Params(num_sites = 10, \n",
    "                    num_rounds = 100,\n",
    "                    inner_epochs = 1,\n",
    "                    batch_size = 64,\n",
    "                    outer_lr = 0.001,\n",
    "                    inner_lr = 0.0001,\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "import pandas as pd\n",
    "A=pd.read_csv('./final_data/UHB.csv')\n",
    "columns2 = [col for col in A.columns if 'Blood_Test' in col]\n",
    "cols=columns2\n",
    "\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "files=['BH','OUH','PUH','UHB']\n",
    "Loaders=get_loaders_structured(cols,files,path='./final_data/',batch=64,unstruct=1)\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "Loaders[0][0].dataset[0][0].shape\n",
    "\n",
    "# %% [code]\n",
    "# Initialise global model and optimizer\n",
    "device = get_device()\n",
    "print(f'Device: {device}')\n",
    "\n",
    "n_features = Loaders[0][0].dataset[0][0].shape[0]\n",
    "\n",
    "\n",
    "hidden_dim=128 \n",
    "in_dim=100\n",
    "\n",
    "# %% [code]\n",
    "# global_model =  DNN(n_features,64,device)\n",
    "global_model= GATT(in_dim, hidden_dim,device)\n",
    "# global_model= GCN(in_dim, hidden_dim,device)\n",
    "\n",
    "global_model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(params=global_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "print(global_model)\n",
    "import time\n",
    "\n",
    "# %% [code]\n",
    "\n",
    "\n",
    "def client_training(global_weights, Loaders, client_id, inner_epochs=25, device=device):\n",
    "\n",
    "    ''' Function to train a client (by its client_id) \"locally\" given the global weights from the server '''\n",
    "    \n",
    "    train_loader = Loaders[client_id][0]\n",
    "\n",
    "    # Build the model (architecture only) locally and initialize is with the global weights\n",
    "    n_features = train_loader.dataset[0][0].shape[0]\n",
    "    # net= GAT(in_dim, hidden_dim,8)\n",
    "    # net = GCNT(in_dim, hidden_dim,device)\n",
    "    # net=Splinconv(in_dim, hidden_dim,device)\n",
    "    net = GATT(in_dim, hidden_dim,device)\n",
    "    # net= APPNPT(in_dim,hidden_dim,device)\n",
    "    net.to(device)\n",
    "    net.load_state_dict(global_weights)      \n",
    "    \n",
    "    # Initialise the local optimizer and store the initial weights\n",
    "    inner_opt = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
    "    inner_state = OrderedDict({k: tensor.data for k, tensor in global_weights.items()})\n",
    "    \n",
    "    # Initialize the local loss function\n",
    "    loss_fn = nn.BCELoss().to(device)\n",
    "    # loss_fcn = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Training loop for local training \n",
    "    train_loss = 0 \n",
    "    \n",
    "        \n",
    "    # Iterate over the batches in the dataloader\n",
    "    for i, batch in enumerate(train_loader):\n",
    "      inputs,label=batch\n",
    "\n",
    "      #************************************************************************\n",
    "      mean_original = inputs.mean()\n",
    "      std_dev_original = inputs.std()\n",
    "      normalized_data = (inputs - mean_original) / std_dev_original\n",
    "\n",
    "\n",
    "      indices = torch.randint(0, normalized_data.size(1), (in_dim-inputs.shape[1],))\n",
    "      resized_data = normalized_data[:, indices]\n",
    "\n",
    "      resized_data=torch.cat((inputs, resized_data),axis=1)\n",
    "      # print(resized_data.shape)\n",
    "      # # Transform back to original scale (optional)\n",
    "      # resized_transformed_data = (resized_data * std_dev_original) + mean_original\n",
    "      #************************************************************************\n",
    "\n",
    "\n",
    "    #   start = time.process_time()\n",
    "      g = MakegraphHT(resized_data)\n",
    "      # feature_transform = FeatureTransform(input_dim=inputs.shape[1], output_dim=16)  # Assume varying input_dim\n",
    "      \n",
    "      \n",
    "      # Move graph and features to device\n",
    "      g = g.to(device)\n",
    "      # transformed_features = feature_transform(g.x.float(),g.edge_index)      \n",
    "      for epoch in range(0, inner_epochs):\n",
    "            if i<len(train_loader)-1:\n",
    "              # print(i,len(train_loader))\n",
    "              # put model in train mode and reset the gradients to zero\n",
    "              net.train()\n",
    "              inner_opt.zero_grad()\n",
    "\n",
    "              # Forward pass\n",
    "              logits = net(g.edge_index, g.x.float())  # note that we pass edge_index and x (node features)\n",
    "              \n",
    "\n",
    "              # load data and labels\n",
    "            #   resized_data=resized_data.to(torch.float32).to(device)\n",
    "              label=label.to(torch.float32).to(device)\n",
    "\n",
    "\n",
    "              # print(logits)\n",
    "              # loss = loss_fn(logits.squeeze(1), label)\n",
    "              loss = loss_fn(logits, label)\n",
    "\n",
    "              # print(loss)\n",
    "              # backward pass to get gradients for optimization\n",
    "              loss.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "              inner_opt.step()\n",
    "\n",
    "\n",
    "              train_loss += loss.detach().cpu().item() \n",
    "\n",
    "\n",
    "    # store gradients of client model after training\n",
    "    final_state = net.state_dict()\n",
    "\n",
    "    # calculate delta theta by subtracting the initial weight from the final weight\n",
    "    delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in global_weights.keys()})\n",
    "    \n",
    "    av_train_loss = train_loss/len(train_loader)\n",
    "    return av_train_loss, delta_theta, net\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# Set model output directory\n",
    "models_dir = 'Ab'\n",
    "if not os.path.exists(f'./trained_models/{models_dir}'):\n",
    "    os.makedirs(f'./trained_models/{models_dir}')\n",
    "\n",
    "if not os.path.exists(f'./Dynamic/{models_dir}'):\n",
    "    os.makedirs(f'./Dynamic/{models_dir}')\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "save_params(f'./trained_models/{models_dir}', parameters)\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "nodes=4\n",
    "# List for storing the metric dataframes for each client\n",
    "DF = [0]*nodes\n",
    "\n",
    "# List for best val auc at each client\n",
    "Val_AUC = [0]*nodes\n",
    "\n",
    "for h in range(0, nodes):\n",
    "    DF[h] = pd.DataFrame(columns=['Train_Loss', 'Val_Loss', 'Val_AUC'])\n",
    "\n",
    "loss_fn = nn.BCELoss().to(device)\n",
    "\n",
    "#### Loop for \"rounds\" of client training\n",
    "num_rounds = 100\n",
    "for i in range(0, num_rounds):\n",
    "    print(f'------------------------------ STARTING TRAINING ROUND: {i+1} ... ---------------------------------') \n",
    "\n",
    "    # List to store the average client training loss after a round of training\n",
    "    TL = [0]*nodes\n",
    "    \n",
    "    # Put global model in training mode and get the weights which will be sent to the clients for local training\n",
    "    global_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    global_weights = global_model.state_dict()\n",
    "\n",
    "    # List to store gradients from each client after a round of training \n",
    "    GRADS = [0]*nodes    \n",
    "    \n",
    "    # List to store the personalised models from each client\n",
    "    NETS = [0]*nodes\n",
    "\n",
    "    # Loop to iteratively train each client locally\n",
    "    for j in range(0, nodes):\n",
    "        TL[j], GRADS[j], NETS[j] = client_training(global_weights, Loaders, j, inner_epochs=1) \n",
    "        print(f'Node : {j}/{nodes} training complete...', end=\"\\r\") \n",
    "\n",
    "    # Combine the grads from each client after a round of local training across all clients to get the meta grad\n",
    "    grad = combine_grads(GRADS) \n",
    "\n",
    "    # Manually update the gradients of the global model parameters using the meta grad \n",
    "    for name, par in global_model.named_parameters():\n",
    "        if par.requires_grad:\n",
    "              par.grad = grad[name]\n",
    "    \n",
    "    # Update the global model parameters by optimizing using the meta grad for SGD\n",
    "    optimizer.step() \n",
    "\n",
    "    # Evaluate the global model (optimized using the meta gradient)\n",
    "    for k in range(0, nodes): \n",
    "        DF[k], Val_AUC[k] = evaluate_modelsT(k, Loaders, NETS[k], TL, loss_fn, device, DF[k], Val_AUC, models_dir)\n",
    "        print(f'Node : {k:.1f} || Val AUC {Val_AUC[k]:.4f}')\n",
    "        \n",
    "    # print(f'\\nNode training loss: {TL[0]:.4f}')\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# Evaluate each client model on its val and test sets\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, nodes): \n",
    "    # global_weights = global_model.load_state_dict(model)\n",
    "    model = torch.load(f'./trained_models/{models_dir}/node{i}')\n",
    "    # global_model.load_state_dict(model)\n",
    "\n",
    "    _, test_auc = prediction_binaryT(model, Loaders[i][2], loss_fn,device) \n",
    "    DF[i].to_csv(f'./Dynamic/{models_dir}/node{i}.csv') \n",
    "    \n",
    "    print(f'Node : {i:.1f} || Val AUC {Val_AUC[i]:.4f} || Test AUC {test_auc:.4f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in global_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "#         print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b705cd38292961bdc2f4c611271080f03114a6b4ba7fc93018366f63f785dc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
